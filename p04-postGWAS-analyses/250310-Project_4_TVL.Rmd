---
title: "HUGEN 2072 Project 4 - Advanced analyses upon GWAS results"
author: "Tianze (Vincent) Luo adapted"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    toc: true
    toc_float: 
        collapsed: no
    df_print: paged
    number_sections: no
    theme: cosmo
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

# Instructions

Below is a guided tutorial to perform some *advanced analyses* building upon the *results from project 3's GWAS of height*.
Some code has been provided to you and some you will need to write yourself.

Project 4 has three components:

-   **Scripting/coding**: read through this file and complete the pipeline by contributing your code *where indicated*.

-   **Narrative**: knit the completed .Rmd to a code-integrated HTML report of your QC process and decisions.

    -   Your report must answer all the questions and perform all the tasks indicated in the instructions/prompts given for each step.

    -   You must submit a successfully-knitted HTML file and the `.Rmd` in order to earn a passing grade for this project.

    -   Set up your report to flow like an actual document you might show your supervisor to summarize the project, emphasizing what was done at each step rather than the code used to do it

    -   **Submit your .Rmd and HTML to Canvas**

-   **Presentation**: you will make a 10-min (maximum) recording of yourself with Panopto in which you summarize this GWAS.
    Instructions will given on Canvas.

In each part of this project you will implement one analysis strategy we learned in class (some steps completed for you).

**Since the code below is incomplete, some of the R code chunks are set to `eval=FALSE` so that knitting to HTML will succeed. You will need to change them to `eval=TRUE` as you progress so that your knitted document shows the results of running each chunk.**

**It is expected that you will copy this file somewhere to your directory on the CRC cluster so that you can use the RStudio Server, where all the packages you need have been installed already.**

**To standardize the starting point of this project, I have provided the necessary results from project 3, so please use those instead of any of your generated files.**

## Part 0 - Setup Workspace and Load Sample Data

First, load the packages we'll be using.

```{r load_packages, message=F, eval=TRUE}
rm(list=ls())

# Load the required packages for the project (Install them if necessary)

library("tidyverse", quietly = TRUE)
library("ggforce", quietly = TRUE)
library("GWASTools", quietly = TRUE)
library("SeqArray", quietly = TRUE)
library("SeqVarTools", quietly = TRUE)
library("GENESIS", quietly = TRUE)
library("GenomicRanges", quietly = TRUE)
library("rsq", quietly = TRUE)
library("EasyQC", quietly = TRUE)
library("qqman", quietly = TRUE)

# Close any open GDS files before proceeding (think of this as starting off with a 'blank slate')

showfile.gds(closeall = T, verbose = T)
```

All of the files you will need are located on the cluster in `/ix1/hugen2072-2025s/p4/`.

```{r part0_setup}

# path to folder for project
shared_folder <- "/ix1/hugen2072-2025s/p4/"

```

## Part 1 - Evaluate GWAS hits

A version of the GWAS from project 3 has been provided for you in the file `p4_study1_gwas_results.txt`.

The columns here are:

-   variant.id: the variant ID from the GDS file

-   chr: chromosome

-   pos: position (hg38)

-   effect_allele: allele coded additively for GWAS

-   other_allele: the other allele at the variant (non-effect allele)

-   imp_r2: imputation quality r-squared

-   freq: the frequency of the effect allele

-   genotyped: `TRUE` if variant was genotyped, `FALSE` if it was imputed

-   n.obs: sample size with non-missing data at that variant

-   MAC: minor allele count

-   Score, Score.SE, Score.Stat, Score.pval, Est, Est.SE, and PVE: columns from `assocTestSingle()` run.

-   strand: strand indicator

The code below reads in these results and gives you an example of how to merge in genotypes for a variant from the results with the phenotype file.
You will need to edit this code where indicated.

```{r part1_gwas}

# read in results from GWAS 1
gwas1 <- read_table(paste0(shared_folder, "p4_study1_gwas_results.txt"))
gwas1

# load sample annotation file
load(paste0(shared_folder, "p4_study1_sample_annotation.RData"), verbose = TRUE)
class(annot)
head(pData(annot))


# open SeqArray GDS file of genetic data from study 1
gds <- seqOpen(paste0(shared_folder, "p4_study1_imputed_data.gds"))
gds


#### Extract genotype for one variant ####
#  this code just does the *first variant in the results file*
# Change my_chr and my_variant to match your variant of interest

my_chr <- "1" # change this to the chromosome you want (keep as character)
my_variant <- 21 # change this to the variant.id you want (keep as numeric)

# Filter gds file to chromosome of interest
seqSetFilterChrom(gds, include=my_chr)

# Additionally filter to variant.id of interest
seqSetFilter(gds, variant.id=my_variant, verbose=TRUE, action = "intersect")

# Get dosage of variant
geno <- alleleDosage(gds, n=0)

############
altChar(gds)        #T is alt allele for var.id=21
head(gwas1, n=1)    #T is non-effect allele from GWAS, effect allele is the ref allele

# check ref allele dosage
head(geno) # HG00096  0

# check genotype 
getGenotype(gds) %>% head() # HG00096 "1|1"

# Conclusion: in this example, we are extracting ***ref-allele dosage*** in `geno`, which is the ***effect allele from GWAS***
############

# Close gds file
seqClose(gds)


# Create a data object with genotypes
geno_df <- as_tibble(geno, rownames = "sample.id")
names(geno_df)[2] <- c("genotype")

# Merge with annotation file - "All rows from x where there are matching values in y"
annot_geno <- inner_join(pData(annot), geno_df, by=c("sample.id"))

# Examine genotype distribution
annot_geno %>% count(genotype) ###[NOTE] "0" is alt/alt


```

## Part 1 - Report

In the results file provided to you, which SNP has the most significant association with height?
Report the GDS variant.id, chromosome, position, alleles, GWAS p-value, GWAS effect size, effect allele, effect allele frequency, and whether the SNP was genotyped or imputed.

- A1.1. In the results file provided, the most significant association is:
    - GDS variant.id = 542766
    - {chr}:{pos}:{effect}_{non-effect} = **19:28492903:C_T**
    - GWAS p-value = 4.21e-06
    - GWAS effect size = -0.88
    - effect allele = C
    - effect allele frequency = 0.87
    - genotyped or imputed -> imputed

<br>

Based on the `locuszoom.png` plot provided to you, how confident do you feel that this signal is *not* a false positive?
Explain.

- A1.2. There is only a **single significant hit** at this locus instead of a tower of hits (variants in high LD), along with the fact that it is an imputed variant despite a high imputed R2. I am confident that this IS a false positive, or I am NOT confident that this is NOT a false positive.  

<br>

Create a sina plot that shows the raw values of height across genotype groups at this top variant.
Comment on whether you think the genotype model you specified is appropriate for this variant or not.

- A1.3. We have plotting a sina plot with height across the Effect/Ref allele (19:28492903:C) dosage groups. There are not any outliers, but given the mean of heights across different dosage groups shown below, it seems that the "additive model" is not appropriate for this variant. A **"dominant/recessive model"** would be more appropriate. 

```
0 (TT)	7.260234	170.5288		
1 (CT)	9.141491	168.0015		
2 (CC)	8.876594	168.2578	
```

<br>

Submit this variant using the Ensemble Variant Effect Predictor web interface (<https://useast.ensembl.org/Tools/VEP>).
Report whether this variant maps to a known rsid and the predicted consequences for this variant.

- A1.4. VEP Input using the following info:
    ```
    > variantInfo(gds)
      variant.id chr      pos ref alt
    1     542766  19 28492903   C   T
    ```
    - Result: 
        - Most severe consequence:   **non_coding_transcript_exon_variant**
        - Colocated variants: **rs45477898**


```{r part1_report}

# Your code goes here

# A1.1. check which SNP has the most significant association with height
target_variant_info = gwas1 %>% arrange(Score.pval) %>% head(1)
target_variant_info


# A1.3. Create a sina plot that shows the raw values of height across genotype groups at this top variant.

## Step1. extract the genotype information
# open gds
gds <- seqOpen(paste0(shared_folder, "p4_study1_imputed_data.gds"))
# filter to that variant
seqSetFilter(gds, variant.id = target_variant_info$variant.id, 
             verbose=TRUE, 
             action = "intersect")

## From GWAS results, we know effect allele is C, but which is ref/alt?
variantInfo(gds) # C is ref -> T is alt
#### Thus, C is ref & effect -> we want to plot by dosage of the effect allele -> dosage of the ref allele ####
## double-checks
# getGenotype(gds) %>% head() #-> T|C
# getGenotypeAlleles(gds) %>% head()

# extract ref-allele dosage
target_ref_allele_dosage = refDosage(gds)
# as.df + rename
df_target_ref_allele_dosage = 
    as.tibble(target_ref_allele_dosage, rownames="sample.id") %>%
    dplyr::rename(refDosage_var542766 = "542766")

df_target_ref_allele_dosage

## Step 2.Merge with phenotype data ("All rows from x where there are matching values in y")
df_pheno_w_dosage = inner_join(pData(annot), #pheno
                                df_target_ref_allele_dosage, #geno dosage
                                by=c("sample.id"))
df_pheno_w_dosage ###[NOTE] "0" is alt/alt

## Step 3. sina-plot shows the raw values of height across genotype groups at this top variant
df_pheno_w_dosage %>% count(refDosage_var542766)

# mutate a df for plotting
df_sina = 
    df_pheno_w_dosage %>% group_by(refDosage_var542766) %>% 
    summarize(sd = sd(height, na.rm=T),
			  mean = mean(height, na.rm=T))
df_sina

ggplot(df_pheno_w_dosage, aes(x=as.factor(refDosage_var542766), y=height)) +
	geom_sina(color="grey") +
	geom_pointrange(aes(x= as.factor(refDosage_var542766),
	                    y=mean, ymin = mean - sd, ymax = mean + sd),
				    data = df_sina,
    				inherit.aes=FALSE) + 
    labs(x = "Effect/Ref allele (19:28492903:C) dosage")


# Close gds file
seqClose(gds)
```

---

## Part 2 - Aggregate Test

A prior study (https://www.nature.com/articles/s41467-024-52579-w#MOESM4) found that variants near the gene **HMGA1** were associated with **greater height**. We are going to test if variants in this region show an effect in our study. 

For reference, the boundaries of the gene HMGA1 are **chr6:34236873-34246231**.

Edit the code below to perform aggregate tests for this **gene region +/- 50kb**. Run a burden test, a SKAT test, and a SKAT-O test. **NOTE: when running multiple tests you need to reset the iterator after each test.** You will need to turn `eval=TRUE` before knitting. 

```{r part2_agg, eval = TRUE}

# Load the null model object to use
load(paste0(shared_folder, "p4_study1_null_model.RData"), verbose=TRUE)

# Get the genotype data
gds <- seqOpen(paste0(shared_folder, "p4_study1_imputed_data.gds"))

# Attach sample annotation
seqData <- SeqVarData(gds, sampleData=annot)
seqData


#### Gene-based Iterator ####
# YOU EDIT: Filter to the chromosome of interest
seqSetFilterChrom(seqData, include = "6")
#check
seqGetData(seqData, "chromosome") %>% unique()

# YOU EDIT: make a GRanges object for this gene region +/- 50kb
    # **chr6:34236873-34246231**
gr <- GRanges(seqnames = 6, 
              ranges = IRanges(start=34236873-50e3, end = 34246231+50e3))


#### Statistical Methods ####
# 1. Run a burden test            
iterator <- SeqVarRangeIterator(seqData, gr)
assoc_burden <- assocTestAggregate(iterator, 
                                   null.model, 
                                   AF.max = 1, #not just rare variants
                                   test="Burden")
seqResetFilter(iterator)

assoc_burden$results
assoc_burden$variantInfo[[1]]


# 2. Run a SKAT test
# your code here
iterator <- SeqVarRangeIterator(seqData, gr)
assoc_SKAT <- assocTestAggregate(iterator, 
                                   null.model, 
                                   AF.max = 1, #not just rare variants
                                   test="SKAT")
seqResetFilter(iterator)

assoc_SKAT$results


# 3. Run a SKAT-O test
# your code here
iterator <- SeqVarRangeIterator(seqData, gr)
assoc_SKATO <- assocTestAggregate(iterator, 
                                   null.model, 
                                   AF.max = 1, #not just rare variants
                                   test="SKATO")
seqResetFilter(iterator)

assoc_SKATO$results


# Close gds file
seqClose(gds)

```

## Part 2 - Report

For each of the three aggregate testing methods (burden, SKAT, and SKAT-O), report the number of variants tested in the region of interest, and the corresponding p-values.

- A2.1. `assoc_burden`, `assoc_SKAT`, `assoc_SKATO`

```{r echo=FALSE, results='asis'}
library(knitr)

data.frame(
    "testing.methods" = c("burden", "SKAT", "SKATO"),
    "number.of.variants"=c(assoc_burden$variantInfo[[1]] %>% nrow(),
                           assoc_SKAT$variantInfo[[1]] %>% nrow(),
                           assoc_SKATO$variantInfo[[1]] %>% nrow()),
    "P-value"=c(assoc_burden$results %>% pull(Score.pval) %>% round(2),
                assoc_SKAT$results %>% pull(pval) %>% round(2),
                assoc_SKATO$results %>% pull(pval_SKATO) %>% round(2))
    ) %>% kable(caption="Aggregate Test Results")

```


Do the results of the three tests come to the same conclusions?

- A2.2. Yes. They all aggregated 4 variants (either rare or common), all of which were non-significant (p>0.05)

Do any of these tests show evidence of association between variants near our gene of interest (HMGA1)?

- A2.3. No. They all yielded non-significant results.

```{r part2_report}

# Your code goes here

# data.frame(
#     "testing.methods" = c("burden", "SKAT", "SKATO"),
#     "number.of.variants"=c(assoc_burden$variantInfo[[1]] %>% nrow(),
#                            assoc_SKAT$variantInfo[[1]] %>% nrow(),
#                            assoc_SKATO$variantInfo[[1]] %>% nrow()),
#     "P-value"=c(assoc_burden$results %>% pull(Score.pval) %>% round(2),
#                 assoc_SKAT$results %>% pull(pval) %>% round(2),
#                 assoc_SKATO$results %>% pull(pval_SKATO) %>% round(2))
#     ) %>% kable(caption="Aggregate Test Results")

```

## Part 3 - PGS

Another study of height generated a polygenic score to predict height (https://www.pgscatalog.org/score/PGS002975/). 


You have been provided with pre-harmonized data for the PGS scoring file `p4_pgs002975_scores.txt` and PLINK formatted data for study 1 `p4_study1.bed|bim|fam`. 

Edit the template code below to point the output to your directory in the `--out` flag.

Then run the code below. At the command line, load PLINK with `module load plink/1.90b6.7`, then run the PLINK command.


```{bash}
ls
echo ""
ls PGS_score_calc
echo ""

cat PGS_score_calc/pgs_calc_slurm.sh

#module load plink/1.90b6.7
#
#plink --bfile /ix1/hugen2072-2025s/p4/p4_study1 \
#    --score /ix1/hugen2072-2025s/p4/p4_pgs002975_scores.txt header sum \
#    --out ./study1_scored 

```

Read the `.profile` file created by PLINK, which contains the values of the PGS and merge with the phenotypes from the annotation object.

**Set this chunk to eval=TRUE once you have updated the file paths.**

```{r part3_pgs, eval = TRUE}

# Change the path to point to where your PLINK output file is
scores <- read_table("./PGS_score_calc/study1_scored.profile",
                     col_names = TRUE,
                     na = "-9") #for plink output
scores

# merge with pheno file
phenotypes <- pData(annot) %>% 
                  inner_join(., 
                             scores %>% select(IID, SCORESUM),
                             by=c("sample.id" = "IID"))

head(phenotypes)

```

## Part 3 - Report

How many SNPs were in the starting PGS scoring files were used here?

- A3.1. **182 SNPs** were in the starting PGS scoring files. 

```{bash}
# score file
wc -l /ix1/hugen2072-2025s/p4/p4_pgs002975_scores.txt
```


How many variants matched the PLINK files for the imputed data?

- A3.2. **181 valid/matched predictors** (However, direct comparison showed 182 matches; maybe it is because some variants have high missing rates in the samples, which were dropped.)

```{bash}
#Q: How many variants matched the PLINK files for the imputed data?

# get variants in pgs file
#awk '{print $1}' /ix1/hugen2072-2025s/p4/p4_pgs002975_scores.txt > ls_pgs_variants.txt

# get variants in plink file
#awk '{print $2}' /ix1/hugen2072-2025s/p4/p4_study1.bim > ls_plink_bim_variants.txt

# compare the two files 
    # -f: extract pattern from file
    # -F: list of pattern to match
    # -x: match whole line
    # -c: count of matches
grep -Fxf PGS_score_calc/ls_pgs_variants.txt -c PGS_score_calc/ls_plink_bim_variants.txt #182

#### However, the log file says 181 valid predictors
cat PGS_score_calc/apply_pgs-slurm_10238.out | grep -e "^--score.*"
```


Based on these results from above applying the PGS for height, evaluate the utility of this PGS in predicting height.
Make sure you report the PGS p-value, phenotypic variance explained (partial r2), and an effect size estimate.

- A3.3. The table of parameters were shown below.

```{r part3_report}

# Your code goes here

## association with height (linear model), adjusting for covariates
eval_lm = lm(height ~ SCORESUM + factor(sex) + factor(pop), data=phenotypes)
summary(eval_lm)


## report: p-value, partial-R2, adjusted-R2, effect size [95%CI]
summary(eval_lm)

#calculate partial R2
partial_r2 = rsq.partial(objF=eval_lm)
partial_r2

#calculate 95% CI
CI_0.95 = confint(eval_lm)[2,] %>% round(2) %>% paste(collapse = ",")

## kable output
data.frame("variable"="SCORESUM",
           "p-value"= 0.19,
           "partial R2"= "0.00067 (0.067%)", 
           "adjusted R2"= 0.73, #accounting for sex and population, total var explained
           "effect size [95%CI]"= paste0("0.68 [",CI_0.95,"]"),
           check.names = FALSE) %>% kable()
```

## Part 4 - File Harmonization Before Meta Analysis

Use the R package EasyQC to perform quality checks for the GWAS from project 3 and for the second GWAS of the height from the UK Biobank (which was already been run).

You can assume the genotypes in both studies have been cleaned (although we will investigate allele frequencies a little).
EasyQC is primarily checking that the GWAS output isn't nonsensical and that alleles are coded consistently between the two studies.

The following files are provided for this part:

-   `p4_study1_gwas_results.txt` - standardized results from the GWAS of height from project 3
-   `p4_study2_gwas_results.txt` - results from a GWAS for height from the UK Biobank (positions are in hg38) (that have been downsampled for you)
-   `p4_gwas1_gwas2.ecf` - **a configuration file for EasyQC**
-   `p4_snp_reference.txt` - an annotation file containing population allele frequencies

The configuration file `p4_gwas1_gwas2.ecf` is almost entirely set up for you.
You only need to make a few changes.
Copy the file `p4_gwas1_gwas2.ecf` to your home directory.

-   First read through the file and the comments.
    The documentation (<https://homepages.uni-regensburg.de/~wit59712/easyqc/EasyQC_9.0_Commands_140918_2.pdf>) also explains how the program works.

-   Specify where to write the output with the `--pathOut` flag (near the top of the file).
    Replace the path with a location within your home directory.

In R, load the EasyQC package and run your configuration script with the command like `EasyQC("gwas1_gwas2.ecf")`, supplying the path to your .ecf file.
This will take a few minutes.

The output includes a log, report (with counts of the number of variants dropped for each cleaning step), some plots, some more files containing information about variants failing some of the filters, and the cleaned data (i.e., GWAS results with problematic variants removed).

Note: everything that you will need to move forward is created as output files.
*So, you can run the next code chunk once and then leave `eval=FALSE` for the chunk when you knit this file.*

Also note that multiple runs of this script file does NOT overwrite the results, it names them with successful counters (e.g., CLEANED.study_1.gz CLEANED_study_1.1.gz).
I recommend you only run this once, otherwise you will need to carefully edit the metal script in part 5 to point to the correct files.

```{r part4_easyqc, eval=FALSE}

# Process the GWAS files with EasyQC
# change this to the path to your folder where you have your copy of the ecf file

# EasyQC("./EasyQC_dir/ecf/p4_gwas1_gwas2.ecf")

```

## Part 4 - Report

Read the cleaned individual GWAS results (`CLEANED.study_1.gz` and `CLEANED.study_2.gz` from EasyQC), and the EasyQC report file (`p4_gwas1_gwas2.rep`) into R.

Summarize the sample sizes for the two individual GWASs.

- A4.1. $N_{\text{study1}} = 2,548$ & $N_{\text{study2}} = 360,388$


How many variants were in each of the two individual GWAS (after EasyQC filtering)?

- A4.2. After EasyQC filters and further cleaning the duplicates (exactly the same rows), **study1 has 104,624 variants** & **study2 has 481,320 variants**.


According to the EasyQC report, how many variants were dropped from each?

- A4.3. According to EasyQC report, for study1, 205	variants were added to it, with (104670-104624) 46 variants further dropped due to duplicated rows. For study2, 6,847 variants were dropped with (482068-481320) 748 variants dropped due to duplicated rows.


How many were flagged for allele frequency discrepancies, for being duplicates, or for failing some other validity check?

- A4.4. Flagged
    - **study1**
        - *n=207*, flagged for allele frequency discrepancies comparing with reference allele frequency (i.e., allele.freq outliers, `AFCHECK.numOutlier`)
        - *n=1*, flagged for being duplicates (`numDuplicates.cptid`)
        - *n=1798*, flagged for other validity checks -> not available in reference file
        
    - **study2**
        - *n=24084*,flagged for allele frequency discrepancies
        - *n=7244*, flagged for being duplicates
        - *n=39670*, flagged for other validity checks -> 31 invalid SE, 2 invalid beta, 41 monomorphic alleles, 39130 not in reference file, 466 mismatches with reference file

```{r part4_report}

# Your code goes here

# Read in 1) CLEANED.study1.gz, 2) CLEANED.study2.gz, #\t delimed
#         3) p4_gwas1_gwas2.rep

study1 = read_delim(file = "./EasyQC_dir/output/CLEANED.study1.gz", #auto-uncompressed
                    col_names = TRUE,
                    # col_types = c("c","i","i", "c","c","c", "d","d","d","d", "i","i"),
                    na = ".") #-- strMissing .

study2 = read_delim(file = "./EasyQC_dir/output/CLEANED.study2.gz", 
                    col_names = TRUE,
                    na = ".")

easyQC_report = read_delim(file = "./EasyQC_dir/output/p4_gwas1_gwas2.rep", #\t
                            col_names = TRUE)
           
#### A4.1. ####
unique(study1$N)
unique(study2$N)

#### A4.2. ####
## Check n_variants for study1
nrow(study1)
study1 %>% select(cptid, STRAND, EFFECT_ALLELE, OTHER_ALLELE) %>% distinct() %>% nrow()

# duplicates?
study1 %>% group_by(cptid, STRAND, EFFECT_ALLELE, OTHER_ALLELE) %>%
    count() %>%
    filter(n>1)
study1 %>% filter(cptid =="10:59402282") #YES

## Remake study1 and study2
# nrow(study1) #104670 before
study1 = study1 %>% distinct()
nrow(study1)

# nrow(study2) #482068 before
study2 = study2 %>% distinct()
nrow(study2)

```

```{r}
#### A4.3. ####

#[NOTE] For duplicated rows, study1 dropped (104670-104624) 46 obs, study2 dropped (482068-481320) 748 obs.

# study1
easyQC_report %>% filter(fileInShortName == "study1") %>%
    mutate(n_drop = numVarIn - numVarOut) %>% 
    select(fileInShortName, n_drop)

# study2
easyQC_report %>% filter(fileInShortName == "study2") %>%
    mutate(n_drop = numVarIn - numVarOut) %>% 
    select(fileInShortName, n_drop)


#### A4.4.####
#Q: How many were flagged for allele frequency discrepancies, for being duplicates, or for failing some other validity check?

### commands ###
# allele frequency discrepancies    -> `AFCHECK`
# duplicated                        -> `CLEANDUPLICATES`

# failing some other validity check -> `CLEAN`, `ADJUSTALLELES`
###          ###


# 1+2: study1 & 2 for allele freq and duplicates
easyQC_report %>% select(fileInShortName, AFCHECK.numOutlier, numDuplicates.cptid)


# 3: study1 - other validity
easyQC_report %>% filter(fileInShortName=="study1") %>%
    select(numDrop_Missing_Alleles, numDrop_Missing_P, numDrop_Missing_BETA, numDrop_Missing_SE, numDrop_Missing_EAF, numDrop_Missing_N, numDrop_invalid_PVAL, numDrop_invalid_SE, numDrop_invalid_BETA, numDrop_invalid_EAF, numDrop_Monomorph, HA.HA.numDrop_BothAllelesMissing, HA.HA.numDrop_InvalidAlleles, 
           NotInRef, #include all AA.*
           AA.AlleleMismatch, AA.AlleleInMissing, AA.AlleleInInvalid, AA.StrandInInvalid) 

# 3: study2 - other validity
easyQC_report %>% filter(fileInShortName=="study2") %>%
    select(numDrop_Missing_Alleles, numDrop_Missing_P, numDrop_Missing_BETA, numDrop_Missing_SE, numDrop_Missing_EAF, numDrop_Missing_N, numDrop_invalid_PVAL, numDrop_invalid_SE, numDrop_invalid_BETA, numDrop_invalid_EAF, numDrop_Monomorph, HA.HA.numDrop_BothAllelesMissing, HA.HA.numDrop_InvalidAlleles, 
           NotInRef,
           AA.AlleleMismatch, AA.AlleleInMissing, AA.AlleleInInvalid, AA.StrandInInvalid) #%>% mutate(sum=sum(.)) %>% select(sum)

```


## Part 5 - Meta Analysis

Use METAL to meta-analyze the two GWAS results.

First, read the documentation to learn how to make a script that meta-analyzes the two studies (<https://genome.sph.umich.edu/wiki/METAL_Documentation>).

The configuration file `metal_script.txt` has been provided.
Read through this file and use the documentation for METAL at the link provided to make sense of what options have been used in this script.

Copy this script to your folder.
Edit the file to provide the paths to the EasyQC output files `CLEANED.study_1.gz` and `CLEANED.study_2.gz`.

At the command line, load metal with `module load metal/2011-03-25`.
To run the meta-analysis use the command `metal metal_script.txt`.
Make sure that you run this from the folder that contains your copy of `metal_script.txt`.

The output will be a log file and a text file with the suffix ".TBL", which contains a table of meta-analysis results.

```         
module load metal/2011-03-25
metal metal_script.txt
```

Note that multiple runs of this script file does NOT overwrite the results, it names them with successful counters (e.g., METAANALYSIS_1.TBL, METAANALYSIS2.TBL).
Be sure you read in the results file you want to by editing the code below.


```{bash}
ls METAL
cat METAL/METAANALYSIS1.TBL.info #show info/log file

## Completed meta-analysis for 519214 markers!
## Smallest p-value is 2.178e-24 at marker '3:141402972'
```

Read the meta-analysis results (the .TBL file) into R and make Manhattan/Q-Q plots for the meta-analysis results.
Edit the code provided below to point to the results file in your directory.
You will need to turn on this chunk prior to knitting by setting `EVAL=TRUE`.

```{r part5_meta, eval=TRUE}

# read in the METAL file
# change the path to your folder where the meta analysis results are
meta <- read_delim("./METAL/METAANALYSIS1.TBL")
head(meta) # Allele1 is the effect allele

# create columns needed for manhattan plot
meta <- meta %>% mutate(chr = str_split_i(MarkerName, ":", 1) %>% as.numeric(),
                        pos = str_split_i(MarkerName, ":", 2) %>% as.numeric())
head(meta)

#### make a manhattan plot ####
library(fastman)
fastman(m = meta, chr="chr", bp="pos", p="P-value", snp = "MarkerName",
        maxP = NULL, #do not allow truncate on Y-value (default=14)
        suggestiveline = -log10(5e-6),
        genomewideline = -log10(5e-8),
        annotationWinMb = 0.5,
        annotateTop = TRUE,
        annotatePval = 5e-8,
        colAbovePval = TRUE,
        # axis size (default=0.6), point size (0.4), label size (0.4)
        cex.axis = 0.8, cex = 0.6, cex.text = 0.6,
        ylim = c(0,25),
        
        annotationAngle=20, #for including a title
        main = "Meta-analysis GWAS of height (suggestive-threshold = 5e-6)")

# # Allow ggrepel
# options(ggrepel.max.overlaps = Inf)
# png( paste0("METAL/meta_manhattan_",format(Sys.time(), "%Y%m%d"),".png"), width=10, height=6, units="in", res=300)
# fastman_gg(m = meta, chr="chr", bp="pos", p="P-value", snp = "MarkerName",
#            logp = TRUE, suggestiveline = -log10(5e-6), genomewideline = -log10(5e-8), maxP=NULL, repel = TRUE,
#            annotatePval = 5e-8, colAbovePval=TRUE, annotateTop=TRUE, annotationWinMb = 0.5, annotationAngle=0,
#            cex=0.9, cex.text=2, cex.axis=0.6, geneannotate = FALSE)
# dev.off()


#### QQ ####
# sort p-values
meta_p = meta %>%
  arrange(`P-value`) %>%
  pull(`P-value`)
range(meta_p) #[1] 2.178e-24 1.000e+00

# png( paste0("METAL/meta_qq_",format(Sys.time(), "%Y%m%d"),".png"),
#     width = 6, height = 6, units = "in", res = 300)
fastqq(meta_p, maxP=NULL) # 1.00327
# dev.off()

# verifying the calculation of lambda, by hand
lambda = qchisq(median(meta_p, na.rm = TRUE), df=1, lower.tail = FALSE) / 
    qchisq(0.5, df=1, lower.tail = FALSE)
### additive coding of SNP (df=1)
### lower.tail = FALSE: because p-value corresponds to the upper-tail cumulative P = P(X>x)
cat( paste0("lambda = ",round(lambda,5),"\n") )



# manhattan(meta,
#           chr = "chr",
#           bp = "pos",
#           snp = "MarkerName",
#           p = "P-value")
# 
# # make a qq plot
# qq(meta$`P-value`)

```

## Part 5 - Report

Based on your reading of the metal script, what method of meta-analysis did we use?

- A5.1. Inverse-variance weighted fixed effect (`SCHEME STDERR`)


Using the meta-analysis results you read in above, do the following:

Make a table summarizing effect directions across the two studies

- A5.2. A table for the effect directions of variants between two studies were made.


How many variants are shared across the two studies (i.e., how many have a non-missing effect and p-value in BOTH studies)?

- A5.3. 66384 variants were shared (no '?' sign in `Direction`)


How many variants are unique to each study?

- A5.4. (Excluding the duplicated ones,) 38240 unique variants in study 1, and 414936 unique variants in study 2.


What percentage of variants present in both studies have the same effect direction in both studies?

- A5.5. Further made a table including only shared variants between studies. Around 50.25% of shared variants have the same effect direction in both studies. 


For the **most significant** variant in the meta-analysis **that was present in both studies**, make a forest plot that shows the two individual study effect estimates and confidence intervals.
Hint: you will need to read in the cleaned files from Easy QC for each study.
**Comment on whether the variant's effects seems to be homogeneous (based on the forest plot**).

**NOTE:** Metal returns Allele1 and Allele2 and the effects and directions it reports are for Allele1.
If you want the opposite allele coding, you need to multiple the effect estimate by `-1`.

- A5.6. Comparing the original summary stats files (`df_info.studies` -> effect allele is "C") and meta-analysis file (`most.sig.shared.variant` -> effect allele is "T"), and given that in METAL the **effect allele is Allele1**, we should flip the sign for effect size (or $\beta$). After more harmonizations, we created a data.frame `meta.results` for making the forest plot. 

- Based on the forest plot, especially the 95% CI, the effect of the variant 20:35384250 (effect allele = C) is **homogeneous** between studies, while the point estimate effect size has opposite directions between studies. 


```{r part5_report}

#### A5.2 ####
xtabs(~ meta$Direction)

#### A5.3 ####
n_shared = xtabs(~ meta$Direction)[c("--","++","-+","+-")] %>% sum() 
n_shared #66384

#### A5.4 ####
nrow(study1) - n_shared
nrow(study2) - n_shared

#### A5.5 ####
proptab_shared_variants = 
    meta %>% filter(Direction %in% c("--","++","-+","+-")) %>% 
    pull(Direction) %>%
    table() %>% prop.table() %>% signif(5)
proptab_shared_variants

proptab_shared_variants[c("--","++")] %>% sum() #0.5025

#### A5.6 ####
# study1 %>% filter(cptid == "19:34984236")
# study2 %>% filter(cptid == "19:34984236")

meta_shared_sortedP = 
    meta %>% filter(Direction %in% c("--","++","-+","+-")) %>% 
    arrange(`P-value`) #sort by P

most.sig.shared.variant = meta_shared_sortedP %>% slice_min(`P-value`)

# make a forest plot that shows the two individual study effect estimates and confidence intervals.
# Hint: you will need to read in the cleaned files from Easy QC for each study.
# Comment on whether the variant's effects seems to be homogeneous (based on the forest plot).

df_info.studies = rbind(
    study1 %>% filter(cptid == most.sig.shared.variant$MarkerName) %>%
        mutate(study="study1"),
    study2 %>% filter(cptid == most.sig.shared.variant$MarkerName) %>%
        mutate(study="study2")
)

df_info.studies
most.sig.shared.variant


#### selecting columns & harmonization & renaming ####
clean_meta_result = 
    most.sig.shared.variant %>% dplyr::select(Effect, StdErr) %>%
        mutate(Effect = -Effect) %>% # effect size for "C" allele
        mutate(study="meta") %>%
        select(study, Effect, StdErr) %>% #clean the column sequence
        dplyr::rename(BETA = Effect, 
               SE = StdErr)
clean_meta_result

clean_studies = df_info.studies %>% select(study, BETA, SE)
clean_studies

# combine all
meta.results = rbind(clean_studies, clean_meta_result)


#### make forest plot for 20:35384250 (effect allele = "C") ####
meta.results <- meta.results %>%
    mutate(l95 = BETA -1.96*SE,
	       u95 = BETA +1.96*SE)

meta.results$study = factor(meta.results$study, levels = c("study1","study2","meta"))
str(meta.results)
meta.results

ggplot(meta.results, aes(y = fct_rev(study)) ) +
    geom_point(aes(x = BETA,
                   size = factor(study == "meta"),
                   shape = factor(study == "meta")) ) +
    
    geom_linerange(aes(xmin = l95, xmax = u95))  +
    geom_vline(xintercept = 0, linetype="dashed") +
    
    scale_size_manual(values = c(2,5)) +    #define dot shape based on if "meta"
    scale_shape_manual(values = c(16,18)) + #define dot shape based on factor 
    theme_classic() +
    theme(legend.position = "none",
            axis.line.y = element_blank(),
            axis.ticks.y= element_blank(),
            axis.title.y= element_blank()) + 
    labs(title = "20:35384250 (effect allele = C)")


```


```{r}
sessionInfo()
```

